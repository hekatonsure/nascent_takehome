# Scripts Function Outline (Post-Refactoring - Nov 2024)

**Status**: 3/4 scripts refactored to tinygrad-inspired style
**Changes**: Functional > OOP (1_quality.py), `__slots__` added (2_repair.py, 3_validation.py), dead code removed
**Total Reduction**: 972+561+420 = 1,953 lines ‚Üí 697+478+396 = 1,571 lines (382 lines removed, -19.6%)

## Quick Reference

| Script | Lines | Style | Functions/Methods | Refactored? |
|--------|-------|-------|-------------------|-------------|
| 1_quality.py | 396 | Functional | 10 private helpers + 3 public API | ‚úÖ Yes |
| 2_repair.py | 697 | Class + `__slots__` | 13 methods | ‚úÖ Yes |
| 3_validation.py | 478 | Class + `__slots__` | 9 methods | ‚úÖ Yes |
| 4_statistics.py | 1,042 | Mixed (functions + class) | 30+ functions/methods | ‚ùå No |

## Script Structure

### 1. `1_quality.py` (396 lines) ‚úÖ REFACTORED
**Purpose**: Detect and flag data quality issues
**Status**: ‚úÖ Active, Required
**Style**: Module-level functions (functional > OOP)

#### Private Helper Functions:
- `_check_sentinel_values(df)` - **[SENTINEL DETECTION]** Detect 1e-08, 1e+20, -10000000000, etc.
- `_check_negative_values(df)` - Detect negative prices/volume/OI
- `_check_ohlc_violations(df)` - Check High >= Low, Open/Close in range
- `_check_missing_data(df)` - Check for nulls
- `_check_duplicates(df)` - Check duplicate (Symbol, Date) pairs
- `_check_stale_prices(df)` - Detect consecutive identical OHLC
- `_check_volume_price_anomalies(df)` - Cross-field validation
- `_temporal_analysis(df, flags)` - Error rates over time
- `_generate_summary_stats(df, flags)` - JSON statistics
- `_print_summary(df, flags)` - Console output

#### Public API Functions:
- `load_data(filepath: str) -> pd.DataFrame` - Load CSV, convert Excel timestamps
- `generate_issue_flags(df: pd.DataFrame) -> pd.DataFrame` - Orchestrate all checks ‚Üí boolean flags
- `main() -> None` - Main execution

#### Outputs:
- `output/data_quality_flags.csv` - Row-level boolean flags (7 issue types)
- `output/data_quality_summary.json` - Statistics (60.8% of rows have issues)

#### Data Flow:
```
data_eng_assessment_dataset.csv
    ‚Üí 1_quality.py
    ‚Üí data_quality_flags.csv (7,450 rows with boolean flags)
    ‚Üí data_quality_summary.json (statistics)
```

---

### 2. `2_repair.py` (697 lines) ‚úÖ REFACTORED
**Purpose**: Conservative data repair (Remove Don't Fabricate)
**Status**: ‚úÖ Active, Required
**Style**: Single class with `__slots__` for memory efficiency

#### Class: `ConservativeDataRepair`
**Attributes (`__slots__`)**:
- `df_original`, `df_cleaned`, `bad_data`, `removal_log`, `repair_log`, `stats`

#### Methods (8-step pipeline):
1. `step_1_drop_missing_timestamps() -> pd.DataFrame` - Remove rows with bad timestamps (662 rows)
2. `step_2_identify_bad_data() -> pd.DataFrame` - **[SENTINEL DETECTION]** Re-detect all issues after step 1
3. `step_3_remove_unfixable_rows() -> None` - Remove OHLC violations (3,120 rows)
4. `step_4_forward_fill_small_gaps(max_gap_days=2, extended_gap_days=5) -> None` - Forward-fill Volume/OI
5. `step_4b_last_traded_price_forward_fill() -> None` - Futures settlement pricing (flat OHLC)
6. `step_4c_cross_contract_relationships() -> None` - Calendar spreads (corr > 0.90)
7. `step_5_remove_remaining_bad_rows() -> None` - Remove remaining bad data
8. `step_6_smart_sparse_symbol_handling() -> None` - Handle FUT11/FUT12 (sparse symbols)
9. `step_7_add_confidence_intervals() -> None` - Add CIs (¬±2 std devs)
10. `step_8_final_validation() -> bool` - Ensure all constraints satisfied
11. `generate_summary() -> dict` - Summary statistics
12. `export_results(output_dir) -> dict` - Save all outputs
13. `run_conservative_repair() -> bool` - Orchestrate full pipeline

#### Main Function:
- `main() -> None` - Entry point

#### Outputs:
- `output/data_cleaned_conservative.csv` - 3,664 rows (49.2% retention)
- `output/data_removal_log.csv` - What was removed and why
- `output/data_repair_log_conservative.csv` - What was repaired (537 forward-fills)
- `output/data_repair_summary_conservative.json` - Statistics

#### Data Flow:
```
data_eng_assessment_dataset.csv (7,450 rows)
    ‚Üí 2_repair.py
    ‚Üí [DOES NOT READ data_quality_flags.csv - re-detects internally]
    ‚Üí data_cleaned_conservative.csv (3,664 rows)
    ‚Üí removal_log.csv, repair_log.csv, summary.json
```

---

### 3. `3_validation.py` (478 lines) ‚úÖ REFACTORED
**Purpose**: Validate data repair quality for trading use
**Status**: ‚úÖ Active, Required
**Style**: Single class with `__slots__` for memory efficiency

#### Class: `ImputationValidator`
**Attributes (`__slots__`)**:
- `dataset_type`, `df_original`, `df_cleaned`, `flags`, `repair_log`, `validation_results`, `output_dir`

#### Methods (6 validation checks):
- `__init__(cleaned_data_path, original_data_path, flags_path, repair_log_path)` - Auto-detect and load datasets
- `check_1_new_violations() -> None` - **[SENTINEL DETECTION]** Re-check for violations after repair
- `check_2_distribution_preservation() -> None` - K-S tests (imputed vs original)
- `check_3_holdout_validation() -> None` - **[PLACEHOLDER]** Not implemented (needs re-run of repair on test set)
- `check_4_look_ahead_bias() -> None` - Detect methods using future data
- `check_5_financial_plausibility() -> None` - Check for suspicious OHLC ranges (flat prices)
- `check_6_symbol_quality() -> None` - Per-symbol trading suitability
- `generate_validation_report() -> None` - Summary: X/6 checks passed
- `run_all_checks() -> None` - Orchestrate all validation

#### Main Function:
- `main() -> None` - Entry point with argparse CLI

#### Outputs:
- `output/validation_results.json` - All checks PASS for conservative data

#### Data Flow:
```
data_cleaned_conservative.csv
+ data_eng_assessment_dataset.csv
+ data_quality_flags.csv
+ data_repair_log_conservative.csv
    ‚Üí 3_validation.py
    ‚Üí validation_results.json (ALL CHECKS PASS ‚úÖ)
```

---

### 4. `4_statistics.py` (1,042 lines) ‚úÖ NOT REFACTORED (Complex)
**Purpose**: Consolidated exploratory analysis suite (3 tools merged)
**Status**: ‚úÖ Active, Optional (Stage 4)
**Style**: Mixed - module functions + class for statistical analysis

#### Section A: Sentinel Value Analysis (lines 35-173)
**Functions:**
- `analyze_sentinel_patterns(df: pd.DataFrame)` - **[SENTINEL DETECTION - REDUNDANT]**
- `print_sentinel_analysis(df: pd.DataFrame)` - Console output with categories
- `print_value_range_summary(df: pd.DataFrame)` - Value ranges for context

**Redundancy**: ‚ö†Ô∏è **YES** - Could read data_quality_flags.csv instead of re-analyzing

#### Section B: Outlier Detection (lines 178-370)
**Functions:**
- `detect_outliers_iqr(series: pd.Series, multiplier: float = 3.0) -> pd.Series` - IQR method
- `detect_outliers_zscore(series: pd.Series, threshold: float = 3.0) -> pd.Series` - Z-score method
- `detect_price_jumps(df: pd.DataFrame, symbol: str, threshold: float = 0.5) -> pd.Series` - Day-over-day jumps >50%
- `analyze_outliers_by_symbol(df: pd.DataFrame)` - Per-symbol analysis (IQR + Z-score)
- `visualize_outliers(df: pd.DataFrame, symbol: str, field: str, output_dir: Path)` - Time series + boxplot

**Outputs:**
- `output/outlier_analysis_results.json`
- `figures/outlier_analysis_{symbol}_{field}.png`

#### Section C: Statistical Analysis (lines 376-930)
**Class:** `StatisticalAnalysis`
**Attributes**:
- `df_cleaned`, `df_original`, `flags`, `output_dir`, `results`

**Methods:**
- `__init__(df_cleaned, df_original, flags)` - Initialize analysis engine
- `analyze_distributions()` - Price/return stats, normality tests (Jarque-Bera)
  - `_plot_price_distributions()` ‚Üí `figures/price_distributions.png`
  - `_plot_return_distributions()` ‚Üí `figures/return_distributions.png`
- `analyze_time_series_properties()` - ACF, stationarity (ADF), volatility
  - `_plot_autocorrelation()` ‚Üí `figures/autocorrelation.png`
  - `_plot_volatility()` ‚Üí `figures/volatility.png`
- `analyze_cross_sectional()` - Correlation matrix, PCA
  - `_plot_correlation_matrix(corr_matrix)` ‚Üí `figures/correlation_matrix.png`
  - `_plot_pca_variance()` ‚Üí `figures/pca_analysis.png`
- `analyze_error_patterns()` - Temporal clustering, chi-square independence
  - `_plot_error_timeline()` ‚Üí `figures/error_timeline.png`
  - `_plot_error_heatmap()` ‚Üí `figures/error_heatmap.png`
- `analyze_microstructure()` - Volume-volatility correlation
- `run_all_analyses()` - Orchestrate all analyses

**Outputs:**
- `output/statistical_analysis_results.json`
- `figures/*.png` (10 visualization files)

#### Main Function (lines 936-1042):
- `main()` - Entry point with argparse CLI
- CLI: `--analysis [sentinel|outliers|statistical|all]`
- Auto-detects paths for root vs scripts/ directory
- Loads original data (for sentinel/outliers) or cleaned data (for statistical)

#### Data Flow:
```
# Sentinel + Outliers:
data_eng_assessment_dataset.csv
    ‚Üí 4_statistics.py --analysis [sentinel|outliers]
    ‚Üí outlier_analysis_results.json
    ‚Üí figures/outlier_*.png

# Statistical:
data_cleaned_conservative.csv
+ data_eng_assessment_dataset.csv
+ data_quality_flags.csv
    ‚Üí 4_statistics.py --analysis statistical
    ‚Üí statistical_analysis_results.json
    ‚Üí figures/*.png (10 files)
```

---

## Redundancy Analysis

### üü¢ **FIXED: Sentinel Value Detection** (Nov 2024)

**Appears in 4 places:**

1. **1_quality.py**: `_check_sentinel_values()` (lines 20-41)
   - ‚úÖ **PRIMARY** - Generates data_quality_flags.csv
   - Detection logic: `< 1e-6`, `> 1e9`, specific values `[1e-08, 1e-10, 1e-20]`, round placeholders `>= 1e10 && % 1e10 == 0`

2. **2_repair.py**: `step_2_identify_bad_data()` (lines 76-79)
   - ‚úÖ **JUSTIFIED** - Re-detects after step 1 drops rows (indices change)
   - Uses same logic as #1
   - **WHY NOT read flags?** After step 1 removes 662 rows, flags file is stale

3. **4_statistics.py**: `analyze_sentinel_patterns()` (lines 35-119)
   - ‚úÖ **FIXED** - Now reads data_quality_flags.csv instead of re-analyzing
   - **Optimization**: Analyzes only 1,452 flagged rows instead of all 7,450 rows (80% reduction)
   - Detailed categorization: scientific small/large, round numbers, negatives, zeros, extremes
   - Backward compatible: Falls back to full analysis if flags file not found

4. **3_validation.py**: `check_1_new_violations()` (lines 88-89)
   - ‚úÖ **JUSTIFIED** - Validation step (ensure repair didn't introduce sentinels)
   - Uses same logic as #1

### üü° **Minor Redundancy: OHLC Violation Checks**

**Appears in 3 places:**
1. 1_quality.py: `check_ohlc_violations()` (lines 83-103)
2. 2_repair.py: `step_2_identify_bad_data()` (lines 117-119)
3. 3_validation.py: `check_1_new_violations()` (lines 128-134)

**Verdict**: ‚úÖ **NOT redundant** - Different stages of pipeline

### üü° **Minor Redundancy: Negative Value Checks**

**Appears in 3 places:**
1. 1_quality.py: `check_negative_values()` (lines 69-80)
2. 2_repair.py: `step_2_identify_bad_data()` (line 109)
3. 3_validation.py: `check_1_new_violations()` (lines 137-144)

**Verdict**: ‚úÖ **NOT redundant** - Different stages of pipeline

### üî¥ **Dead Code: Outlier Repair (100+ lines)**

**Location**: 2_repair.py: `step_2b_repair_statistical_outliers()` (lines 142-269)

**Status**: ‚ö†Ô∏è **DISABLED** - Commented out at line 914 in `run_conservative_repair()`

**Reason**: Creates more violations than it fixes (378 violations for 308 extra rows)

**RECOMMENDATION**:
- Remove from production code (archive in separate branch if needed)
- Document lesson learned in CLAUDE.md (already done)

---

## Connection Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ data_eng_assessment_dataset.csv (7,450 rows)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [STAGE 1] 1_quality.py
         ‚îÇ           ‚îÇ
         ‚îÇ           ‚îú‚îÄ‚Üí data_quality_flags.csv (7,450 rows with boolean flags)
         ‚îÇ           ‚îî‚îÄ‚Üí data_quality_summary.json (60.8% have issues)
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [STAGE 2] 2_repair.py
         ‚îÇ           ‚îÇ  (Does NOT read flags - re-detects internally)
         ‚îÇ           ‚îÇ
         ‚îÇ           ‚îú‚îÄ‚Üí data_cleaned_conservative.csv (3,664 rows, 49.2% retention)
         ‚îÇ           ‚îú‚îÄ‚Üí data_removal_log.csv (3,786 rows removed)
         ‚îÇ           ‚îú‚îÄ‚Üí data_repair_log_conservative.csv (537 forward-fills)
         ‚îÇ           ‚îî‚îÄ‚Üí data_repair_summary_conservative.json
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [STAGE 3] 3_validation.py
         ‚îÇ           ‚îÇ  (Reads: cleaned, original, flags, repair_log)
         ‚îÇ           ‚îÇ
         ‚îÇ           ‚îî‚îÄ‚Üí validation_results.json (ALL CHECKS PASS ‚úÖ)
         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [STAGE 4 - OPTIONAL] 4_statistics.py
                     ‚îÇ  (--analysis [sentinel|outliers|statistical|all])
                     ‚îÇ
                     ‚îú‚îÄ‚Üí Sentinel: Uses original data (REDUNDANT - could use flags)
                     ‚îú‚îÄ‚Üí Outliers: Uses original data
                     ‚îî‚îÄ‚Üí Statistical: Uses cleaned + original + flags
                         ‚îÇ
                         ‚îú‚îÄ‚Üí statistical_analysis_results.json
                         ‚îú‚îÄ‚Üí outlier_analysis_results.json
                         ‚îî‚îÄ‚Üí figures/*.png (10 visualizations)
```

---

## Shared Logic (No Utility Module)

**Common detection patterns NOT extracted:**
- Sentinel value detection (4 places)
- OHLC constraint checks (3 places)
- Negative value checks (3 places)
- Missing data checks (2 places)

**WHY no shared module?**
- Each script is standalone/runnable independently
- Different scripts need detection at different stages
- Indices change between stages (after removals)

**RECOMMENDATION**: Consider creating `utils.py` with:
```python
def detect_sentinel_values(series: pd.Series) -> pd.Series:
    """Reusable sentinel detection (< 1e-6, > 1e9, etc.)"""
    return (
        ((series > 0) & (series < 1e-6)) |
        (series.abs() > 1e9) |
        series.isin([1e-08, 1e-10, 1e-20]) |
        ((series.abs() >= 1e10) & ((series % 1e10) == 0))
    )

def check_ohlc_constraints(df: pd.DataFrame) -> dict:
    """Reusable OHLC constraint checks"""
    ...
```

---

## Recommendations

### 1. ‚úÖ Keep Current Structure
**WHY**: Pipeline stages are independent and need fresh detection after transformations

### 2. ‚ö†Ô∏è Remove Dead Code
**File**: 2_repair.py
**Lines**: 142-269 (`step_2b_repair_statistical_outliers()`)
**Action**: Delete or move to separate `archive/` directory
**Reason**: Disabled since Nov 2024, creates more problems than it solves

### 3. ‚úÖ Reduce Redundancy in 4_statistics.py (COMPLETED Nov 2024)
**File**: 4_statistics.py
**Function**: `analyze_sentinel_patterns()` (lines 35-119)
**Action**: ‚úÖ Now reads `data_quality_flags.csv` instead of re-analyzing original data
**Benefit**: 80% reduction in work (1,452 flagged rows vs 7,450 total rows), consistent with Stage 1 detection

### 4. üîµ Consider Creating Shared Utils (Optional)
**File**: `scripts/utils.py` (new)
**Contents**:
- `detect_sentinel_values(series)`
- `check_ohlc_constraints(df)`
- `check_negative_values(df)`
**Benefit**: DRY principle, consistent detection logic
**Trade-off**: Adds coupling between scripts

### 5. ‚úÖ Keep Pipeline Independence
**WHY**: Each script can run standalone, doesn't break if upstream changes

---

## Summary (Post-Refactoring - Nov 2024)

### Active Scripts: 4
1. ‚úÖ 1_quality.py (396 lines) - Detection | **REFACTORED** to functional style
2. ‚úÖ 2_repair.py (697 lines) - Repair | **REFACTORED** with `__slots__`, no dead code
3. ‚úÖ 3_validation.py (478 lines) - Validation | **REFACTORED** with `__slots__`
4. ‚úÖ 4_statistics.py (1,042 lines) - Exploratory | **NOT REFACTORED** (complex, mixed style)

### Refactoring Improvements:
- ‚úÖ **1_quality.py**: Converted to functional style (private `_func()` helpers + public API)
- ‚úÖ **2_repair.py**: Added `__slots__`, removed dead code (was 972 ‚Üí now 697 lines)
- ‚úÖ **3_validation.py**: Added `__slots__`, simplified (was 561 ‚Üí now 478 lines)
- ‚è∏Ô∏è **4_statistics.py**: Not refactored (too complex, works well as-is)

### Dead Code: REMOVED ‚úÖ
- ~~2_repair.py: `step_2b_repair_statistical_outliers()` (100+ lines)~~ **REMOVED**

### Redundancy: FIXED ‚úÖ (Nov 2024)
- ‚úÖ 4_statistics.py: Sentinel detection now uses flags CSV (no longer duplicates Stage 1)
  - **Fix**: Added `flags_path` parameter to `analyze_sentinel_patterns()` and `print_sentinel_analysis()`
  - **Optimization**: 80% reduction in work (1,452 flagged rows vs 7,450 total rows)
  - **Impact**: Significant performance improvement for optional Stage 4 analysis

### Connections:
- **2_repair.py does NOT read data_quality_flags.csv** (re-detects internally after step 1 drops rows)
- **3_validation.py reads all outputs** (cleaned, original, flags, repair_log)
- **4_statistics.py reads cleaned + original + flags** (statistical mode)

### Code Style Evolution:
- **1_quality.py**: Pure functional (tinygrad-inspired)
- **2_repair.py**: Class with `__slots__` (memory-conscious)
- **3_validation.py**: Class with `__slots__` (memory-conscious)
- **4_statistics.py**: Mixed (functions + class, not refactored)

### Verdict:
- ‚úÖ **Well-structured** - Each stage is independent
- ‚úÖ **Dead code removed** - Outlier repair function deleted
- ‚úÖ **Memory efficient** - Classes use `__slots__`
- ‚úÖ **Functional where appropriate** - 1_quality.py uses module functions
- ‚ö†Ô∏è **Minor redundancy** - Sentinel detection in exploratory (justifiable)
- ‚úÖ **No shared utils needed** - Detection logic duplication is intentional (pipeline independence)
